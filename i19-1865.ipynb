{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ali Kamal\n",
    "# i191865@nu.edu.pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import difflib\n",
    "import string\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gets a text file as input and returns a \n",
    "stripped list containing the text\n",
    "\"\"\"\n",
    "def read(f):\n",
    "    with open(f) as file:\n",
    "        return [(line.rstrip()) for line in file.readlines() if line.strip()]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Gets a list of text as input, and returns\n",
    "ngram (bigram,trigram,4-gram etc) depending\n",
    "on input parameters\n",
    "\"\"\"\n",
    "#1. Train a uni-gram model using the corpus provided in data.txt.\n",
    "def get_ngrams(text,ngram=1):\n",
    "    words=[word for word in text.split(\" \")]\n",
    "    return [' '.join(ngram) for ngram in zip(*[words[i:] for i in range(0,ngram)])]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Gets a list of ngrams and returns \n",
    "a dictionary with ngrams as keys\n",
    "and their counts in the corpus as values\n",
    "\"\"\"\n",
    "def get_ngram_counts(ngrams):\n",
    "    return Counter(np.hstack(ngrams))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Increments edit table dictionary, or \n",
    "makes a new entry on the dictionary\n",
    "if they key does not exist\n",
    "\"\"\"\n",
    "def increment_dic(dic,correct,wrong):\n",
    "    try:\n",
    "        dic[correct+\",\"+wrong]+=1\n",
    "    except KeyError:\n",
    "        dic[correct+\",\"+wrong]=1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Takes misspelings text file as input and\n",
    "generates respective edit tables based on \n",
    "the manner of misspellings (insert,delete,substitute,transpose)\n",
    "\"\"\"\n",
    "#2. Create insert, delete, substitution and transposition tables for alphabets a-z\n",
    "#   using the provided misspellings.txt. The tables can be implemented using Python dictionaries\n",
    "def generate_edit_tables(data,insert=dict(),delete=dict(),substitute=dict(),transpose=dict()):\n",
    "    with open(data) as file:\n",
    "        next(file)\n",
    "        for line in file.readlines():\n",
    "            separated_line=line.split()\n",
    "            correct_word=re.sub(\",\",\"\",separated_line[0])\n",
    "            wrong_words=separated_line[1:]\n",
    "            for idx,wrong_word in enumerate(wrong_words):\n",
    "                if(len(wrong_word)!=len(correct_word)): #Insert,Delete Error\n",
    "                    if(len([elem for elem in list(difflib.ndiff(correct_word,wrong_word))if('+'in elem or'-'in elem)])==1):\n",
    "                        for idx2,i in enumerate(difflib.ndiff(correct_word, wrong_word)):\n",
    "                            if i[0]==' ': #No difference\n",
    "                                continue\n",
    "                            elif i[0]=='+': #Insert Error\n",
    "                                #X=>XY (X=Correct,Y=Incorrect), dictionary like this: dic[X,Y]\n",
    "                                increment_dic(insert,'@',i[-1]) if idx2==0 else increment_dic(insert,correct_word[idx2-1],i[-1])\n",
    "                                #@ represents start of string\n",
    "                            elif i[0]=='-': #Delete Error\n",
    "                                #XY=>X (X=Correct,Y=Correct), dictionary like this: dic[X,Y]\n",
    "                                increment_dic(delete,'@',i[-1]) if idx2==0 else increment_dic(delete,correct_word[idx2-1],i[-1])\n",
    "                else: #Substitute,Transpose Error\n",
    "                    if(len([elem for elem in list(difflib.ndiff(correct_word,wrong_word))if('+'in elem or'-'in elem)])==2):\n",
    "                        for idx,i in enumerate(wrong_word):\n",
    "                            if(i!=correct_word[idx]):       \n",
    "                                if(idx!=len(wrong_word)-1 and wrong_word[idx+1]==correct_word[idx]): #Transpose Error                           \n",
    "                                    #XY=>YX (XY=Correct,YX=Incorrect), dictionary like this: dic[X,Y]\n",
    "                                    increment_dic(transpose,correct_word[idx],i)\n",
    "                                else:\n",
    "                                    #X=>Y (X=Correct,Y=Incorrect), dictionary like this: dic[Y,X]\n",
    "                                    increment_dic(substitute,i,correct_word[idx])                 \n",
    "    return dict(sorted(insert.items())),dict(sorted(delete.items())),dict(sorted(substitute.items())),dict(sorted(transpose.items()))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Returns error model probabilites P(x|w) with \n",
    "laplace smoothing, corresponding to the \n",
    "respective error model table\n",
    "\"\"\"\n",
    "#3. Create a function that calculates P(x|w) using the Error Model tables.\n",
    "def prob_error_model(wrong_word,idx,error_model_table,vocab_len,op,correct_word=None):\n",
    "    #Calculating probabilities WITH laplace smoothing\n",
    "    try:\n",
    "        if(idx==0 and op=='insert'):\n",
    "            return (error_model_table['@'+','+wrong_word[idx]]+1)/(sum(error_model_table.values())+vocab_len)\n",
    "        elif(idx==0 and op=='delete'):\n",
    "            return (error_model_table['@'+','+correct_word]+1)/(sum(error_model_table.values())+vocab_len)\n",
    "        elif(idx>0 and op=='insert'):\n",
    "            return (error_model_table[wrong_word[idx-1]+','+wrong_word[idx]]+1)/(sum(error_model_table.values())+vocab_len)\n",
    "        elif(idx>0 and op=='delete'):\n",
    "            return (error_model_table[wrong_word[idx-1]+','+correct_word]+1)/(sum(error_model_table.values())+vocab_len)\n",
    "        elif(op=='substitute'):\n",
    "            return (error_model_table[wrong_word[idx]+','+correct_word]+1)/(sum(error_model_table.values())+vocab_len)\n",
    "        elif(op=='transpose'):\n",
    "            return (error_model_table[wrong_word[idx+1]+','+wrong_word[idx]]+1)/(sum(error_model_table.values())+vocab_len)\n",
    "    except KeyError:\n",
    "        return (1)/(sum(error_model_table.values())+vocab_len)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Takes an incorrect word and generates\n",
    "correct candidate words for it, provided they are\n",
    "within 1 edit distance(insert,delete,transpose,substitute).\n",
    "\"\"\"\n",
    "#4. Create a function that returns the set of candidate correct words for a given word x. The\n",
    "#   set of candidatewords must be a subset of the vocabulary V . This means that for each\n",
    "#   candiate word w, P(w) > 0.\n",
    "def generate_candidate_words(wrong_word,unigrams,insert,delete,substitute,transpose,k):\n",
    "    vocab_len=len(unigrams.keys())\n",
    "    len_word=len(wrong_word)\n",
    "    ins=dict()\n",
    "    dl=dict()\n",
    "    sub=dict()\n",
    "    trans=dict()\n",
    "    for idx,i in enumerate(wrong_word):\n",
    "        modified_word_ins=wrong_word[:idx]+''+ wrong_word[idx + 1:]\n",
    "        if(idx!=len_word-1 and len_word>1):\n",
    "            modified_word_trans=''.join((wrong_word[:idx],wrong_word[idx+1],wrong_word[idx+1:idx+1],wrong_word[idx],wrong_word[idx+2:]))\n",
    "            #transpose (if swapping of two adjacent characters from wrong_word results in candidate word present in Vocabulary)\n",
    "            if(modified_word_trans in unigrams.keys()):\n",
    "                trans[modified_word_trans]=prob_error_model(wrong_word,idx,transpose,vocab_len,'transpose')\n",
    "        #insert (if deletion of word from wrong_word results in candidate word present in Vocabulary)\n",
    "        if(modified_word_ins in unigrams.keys()):\n",
    "            ins[modified_word_ins]=prob_error_model(wrong_word,idx,insert,vocab_len,'insert')\n",
    "        for c in list(map(chr,range(ord('a'),ord('z')+1))):\n",
    "            if(c!=i):\n",
    "                modified_word_sub=wrong_word[:idx] + c + wrong_word[idx + 1:]\n",
    "                #substitute (if substitution of word in wrong_word results in candidate word resent in Vocabulary)\n",
    "                if(modified_word_sub in unigrams.keys()):\n",
    "                    sub[modified_word_sub]=prob_error_model(wrong_word,idx,substitute,vocab_len,'substitute',c)\n",
    "            modified_word_dl=wrong_word[:idx] + c + wrong_word[idx:]\n",
    "            #delete (if insertion of word in wrong_word results in candidate word resent in Vocabulary)\n",
    "            if(modified_word_dl in unigrams.keys()):\n",
    "                dl[modified_word_dl]=prob_error_model(wrong_word,idx,delete,vocab_len,'delete',c)\n",
    "    #returns top k candidate words of each error type having the most probability\n",
    "    return dict(sorted(ins.items(),key=itemgetter(1),reverse=True)[:k]),dict(sorted(dl.items(),key=itemgetter(1),reverse=True)[:k]),dict(sorted(sub.items(),key=itemgetter(1),reverse=True)[:k]),dict(sorted(trans.items(),key=itemgetter(1),reverse=True)[:k])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Takes a sentence as input and and generates all combinations\n",
    "of candidate words and the original sentence\n",
    "\"\"\"\n",
    "#5. Create a function that returns a list of candidate correct sentences \n",
    "#   created using all combinations of your candidate correct words and the original sentence.\n",
    "def generate_candidate_sentences(sentence,unigrams,insert,delete,substitute,transpose,k):\n",
    "    sentence=sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    sentence = sentence.split() if isinstance(sentence, str) else sentence\n",
    "    candidate_sentences=dict()\n",
    "    probs=dict()\n",
    "    for s in sentence:\n",
    "        ins,dl,sub,trans=generate_candidate_words(s,unigrams,insert,delete,substitute,transpose,k)\n",
    "        candidate_sentences.setdefault(s,[]).append(s)#8. Your solution must also be able to handle cases\n",
    "                                                      #   when there are no candidate words.\n",
    "        [candidate_sentences.setdefault(s,[]).append(key) for key in ins.keys()]\n",
    "        [candidate_sentences.setdefault(s,[]).append(key) for key in dl.keys()]\n",
    "        [candidate_sentences.setdefault(s,[]).append(key) for key in sub.keys()]\n",
    "        [candidate_sentences.setdefault(s,[]).append(key) for key in trans.keys()]\n",
    "        probs.update(ins)\n",
    "        probs.update(dl)\n",
    "        probs.update(sub)\n",
    "        probs.update(trans)\n",
    "    #Gets all combinations of candidate correct words, with a limit of 1 MILLION combinations max, in order to improve \n",
    "    #compute performance. Else, we quickly run out of RAM\n",
    "    candidates=itertools.islice(itertools.product(*(candidate_sentences[key] for key in candidate_sentences)),10000000)\n",
    "    return list(candidates),probs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Returns probability of candidate sentence \n",
    "w.r.t error model tables\n",
    "\"\"\"\n",
    "def get_error_model_probs(candidates,error_model_probs):\n",
    "    probabilities=[]\n",
    "    for sen in candidates:\n",
    "        prob=0\n",
    "        for word in sen:\n",
    "            try:\n",
    "                prob+=error_model_probs[word]\n",
    "            except:\n",
    "                pass\n",
    "        probabilities.append(prob)\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Returns probability of candidate sentence \n",
    "w.r.t bigram model\n",
    "\"\"\"\n",
    "def get_bigram_probs(candidates,bigrams,error_model_probs,vocab_len):\n",
    "    probabilities=[]\n",
    "    laplace_denom=sum(bigrams.values())+vocab_len\n",
    "    bigrams_prob=dict()\n",
    "    for sen in candidates:\n",
    "        prob=0\n",
    "        for idx,word in enumerate(sen):\n",
    "            try:\n",
    "                if(not idx):#First Word\n",
    "                    prob+=error_model_probs[word]\n",
    "                else:\n",
    "                    try:\n",
    "                        prob+=bigrams_prob[sen[idx-1]+\" \"+word]\n",
    "                    except KeyError:\n",
    "                        bigrams_prob[sen[idx-1]+\" \"+word]=(bigrams[sen[idx-1]+\" \"+word]+1)/laplace_denom\n",
    "                        prob+=bigrams_prob[sen[idx-1]+\" \"+word]\n",
    "            except:\n",
    "                pass\n",
    "        probabilities.append(prob)\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Returns probabilities of candidate sentences generated\n",
    "through bigram model and error model table\n",
    "\"\"\"\n",
    "#6. Train an ensemble of atleast one character-level language model and atleast\n",
    "#   one word-level language model using the corpus provided in data.txt.\n",
    "def ensemble_model(candidates,error_model_probs,unigrams,bigrams):\n",
    "    vocab_len=len(unigrams.keys())\n",
    "    #Getting probabilities of candidate sentences w.r.t edit tables\n",
    "    em=get_error_model_probs(candidates,error_model_probs)\n",
    "    #Getting probabilities of candidate sentences w.r.t bigram\n",
    "    bp=get_bigram_probs(candidates,bigrams,error_model_probs,vocab_len)\n",
    "    return np.array(bp),np.array(em)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns candidate sentence which has highest final probability,\n",
    "after getting probabilities through ensemble model\n",
    "\"\"\"\n",
    "#7. Create a function that takes a list of candidate correct sentences as input\n",
    "#   and returns the candidate correct sentence with the highest probability\n",
    "#   according to your ensemble of language models.\n",
    "def get_best_candidate_sentence(candidates,error_model_probs,unigrams,bigrams):\n",
    "    bp,em=ensemble_model(candidates,error_model_probs,unigrams,bigrams)\n",
    "    #Adding both probabilities, one from bigrams and the other from error model tables, with some \n",
    "    #parameters to reduce impact of error model probabilities, giving more weight to bigram probabilities,\n",
    "    #but not completely disregarding error modle probabilities\n",
    "    final_prob=bp + (em/4)\n",
    "    #returning argmax of highest probabilities\n",
    "    return \" \".join(candidates[np.argmax(final_prob)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating unigrams and bigrams\n",
    "data=read(\"data.txt\")\n",
    "unigrams=[]\n",
    "bigrams=[]   \n",
    "for i in data:\n",
    "    unigrams.append(get_ngrams(i))\n",
    "    bigrams.append(get_ngrams(i,2))\n",
    "unigrams=get_ngram_counts(unigrams)\n",
    "bigrams=get_ngram_counts(bigrams)\n",
    "#Deleting bigram keys which have count of only 1, to reduce memory footprint.\n",
    "#Keys with counts of 1 are not that important to us, plus we are already using laplace smoothing,\n",
    "#so even if we do need a key(in the future) that we are deleting, this should be handled by laplace smoothing.\n",
    "[bigrams.pop(k) for k,v in list(bigrams.items()) if v==1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating error model edit tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating respective Error Model Edit Tables\n",
    "insert,delete,substitute,transpose=generate_edit_tables(\"misspellings.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating candidate sentences from candidate words for sample incorrect sentence, and getting best candidate sentence according to our ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating all combinations of candidate words from original sentence\n",
    "\n",
    "incorrect_sentence='mvin pcakistan mevn rwhta houqn eor scahool wfaloun xke savh NLP parhnla hchahta hounp'\n",
    "\n",
    "#Gets top k candidate words according to error model probability. Getting top k in order to improve \n",
    "#compute performance, else we quickly run out of RAM\n",
    "k=5\n",
    "candidates,error_model_probs=generate_candidate_sentences(incorrect_sentence,unigrams,insert,delete,substitute,transpose,k)\n",
    "\n",
    "#Gets best candidate sentence according to our ensemble model\n",
    "correct_sentence=get_best_candidate_sentence(candidates,error_model_probs,unigrams,bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing our chosen best candidate sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correct_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
